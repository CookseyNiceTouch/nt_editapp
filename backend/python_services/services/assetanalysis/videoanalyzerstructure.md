# VideoAnalyzer JSON Structure

This document describes the structure and format of JSON files produced by the VideoAnalyzer tool, designed to be easily parsed by AI systems.

## Overview

The JSON output represents a frame-accurate transcription of a video file with speaker detection and silence marking. Transcription is performed by AssemblyAI's API with accurate word-level timestamps that are converted to frame numbers for video editing applications. The enhanced version includes explicit silence detection to identify gaps, pauses, and potential editing cut points.

## Top-Level Structure

```json
{
  "file_name": "20250509_MTC_2206.MP4",
  "fps": 25.0,
  "duration_frames": 26616,
  "speakers": ["A", "B"],
  "full_transcript": "I do, because I just. I got too far ahead of myself. Oh, yeah, yeah. Sometimes you're like, oh, what am I saying? Next...",
  "silence_threshold_ms": 1000,
  "words": [
    {
      "word": "I",
      "speaker": "A",
      "frame_in": 2,
      "frame_out": 5,
      "confidence": 0.99
    },
    {
      "word": "do,",
      "speaker": "A",
      "frame_in": 5,
      "frame_out": 10,
      "confidence": 0.95
    },
    {
      "word": "**SILENCE**",
      "speaker": "**SILENCE**",
      "frame_in": 45,
      "frame_out": 70,
      "confidence": 1.0,
      "duration_ms": 1000
    },
    {
      "word": "because",
      "speaker": "A",
      "frame_in": 70,
      "frame_out": 85,
      "confidence": 0.98
    }
    // Additional words and silence markers...
  ]
}
```

## Field Descriptions

### Metadata Fields

| Field | Type | Description |
|-------|------|-------------|
| `file_name` | string | The original filename of the processed video |
| `fps` | number (float) | Frames per second of the video |
| `duration_frames` | number (integer) | Total number of frames in the video |
| `speakers` | array of strings | List of unique speaker identifiers found in the transcript (typically "A", "B", etc.) |
| `full_transcript` | string | Complete text transcript of the entire video |
| `silence_threshold_ms` | number (integer) | Threshold in milliseconds for silence detection |

### Words Array

The `words` array contains objects representing individual words detected in the transcript, as well as silence markers. Each entry has the following properties:

| Field | Type | Description |
|-------|------|-------------|
| `word` | string | The transcribed word or "**SILENCE**" for silence markers |
| `speaker` | string | Speaker identifier (e.g., "A", "B") or "**SILENCE**" for silence markers |
| `frame_in` | number (integer) | Starting frame where the word/silence begins |
| `frame_out` | number (integer) | Ending frame where the word/silence ends |
| `confidence` | number (float) | Confidence score of the word detection (1.0 for silence markers) |
| `duration_ms` | number (integer) | Duration in milliseconds (only present for silence markers) |

## Frame Calculation

Frame numbers are calculated from AssemblyAI's millisecond timestamps using the formula:

```
frame_in = round((start_time_ms / 1000) * fps)
frame_out = round((end_time_ms / 1000) * fps)
```

If `frame_out` equals `frame_in` (for very short words), `frame_out` is set to `frame_in + 1` to ensure a minimum word duration of one frame.

## Speaker Detection

Speaker labels are generated by AssemblyAI's speaker diarization technology. The system:

1. Analyzes the audio to identify different speakers
2. Assigns each speaker a unique identifier (typically "A", "B", "C", etc.)
3. Tags each word with its corresponding speaker identifier

For example, in a conversation between two people:

```json
[
  {
    "word": "Hi,",
    "speaker": "A",
    "frame_in": 294,
    "frame_out": 303
  },
  {
    "word": "I'm",
    "speaker": "A",
    "frame_in": 303,
    "frame_out": 309
  },
  {
    "word": "Cooksey.",
    "speaker": "A",
    "frame_in": 309,
    "frame_out": 326
  },
  {
    "word": "And",
    "speaker": "B",
    "frame_in": 342,
    "frame_out": 349
  },
  {
    "word": "I'm",
    "speaker": "B",
    "frame_in": 349,
    "frame_out": 355
  },
  {
    "word": "Paul.",
    "speaker": "B",
    "frame_in": 355,
    "frame_out": 364
  }
]
```

## Custom Spelling

The VideoAnalyzer supports custom spelling corrections via AssemblyAI's custom_spelling feature. This allows you to specify:

```json
[
  {
    "from": ["misspelled", "mispelled"],
    "to": "misspelled"
  }
]
```

## Error Format

If processing fails, the JSON will have a different structure:

```json
{
  "error": "Error message describing what went wrong",
  "timestamp": "2023-10-01T15:00:00Z"
}
```

## Parsing Guidelines for AI

1. **Speaker Consistency**: All entries in the `words` array have speaker values that match entries in the `speakers` array or are "**SILENCE**".

2. **Frame Range Validation**: All frame numbers are non-negative and less than `duration_frames`.

3. **Sequential Word Timing**: Words and silence markers appear in chronological order, with `frame_in` values generally increasing.

4. **Speaker Groups**: Words from the same speaker are often grouped together in the `words` array.

5. **Transcript Reconstruction**: The complete transcript is provided in `full_transcript`, but you can also reconstruct it from the words array (excluding silence markers).

6. **Silence Handling**: 
   - **Exclude from segments**: Do not include `**SILENCE**` entries in final speech segments
   - **Use for boundaries**: Silence markers indicate natural break points for editing
   - **Large gaps**: Silence longer than 2-3 seconds often indicates separate takes or retakes
   - **Cut point identification**: Silence markers help identify optimal places for video cuts

7. **Confidence Scores**: Use confidence values to identify potentially misrecognized words (silence markers always have confidence 1.0).

## JSON Schema

For validation purposes, the following JSON Schema can be used:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["file_name", "fps", "duration_frames", "speakers", "full_transcript", "words"],
  "properties": {
    "file_name": {
      "type": "string",
      "description": "Original filename of the video"
    },
    "fps": {
      "type": "number",
      "description": "Frames per second of the video"
    },
    "duration_frames": {
      "type": "integer",
      "description": "Total number of frames in the video"
    },
    "speakers": {
      "type": "array",
      "items": {
        "type": "string"
      },
      "description": "List of unique speaker identifiers"
    },
    "full_transcript": {
      "type": "string",
      "description": "Complete transcript text"
    },
    "silence_threshold_ms": {
      "type": "integer",
      "description": "Threshold in milliseconds for silence detection",
      "minimum": 100,
      "default": 1000
    },
    "words": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["word", "speaker", "frame_in", "frame_out"],
        "properties": {
          "word": {
            "type": "string",
            "description": "The transcribed word or **SILENCE** for silence markers"
          },
          "speaker": {
            "type": "string",
            "description": "Speaker identifier or **SILENCE** for silence markers"
          },
          "frame_in": {
            "type": "integer",
            "minimum": 0
          },
          "frame_out": {
            "type": "integer",
            "minimum": 0
          },
          "confidence": {
            "type": "number",
            "minimum": 0,
            "maximum": 1,
            "description": "Confidence score of word detection"
          },
          "duration_ms": {
            "type": "integer",
            "minimum": 0,
            "description": "Duration in milliseconds (only for silence markers)"
          }
        }
      }
    }
  },
  "additionalProperties": false
}
```

## Silence Detection

The VideoAnalyzer automatically detects and marks periods of silence longer than the specified threshold. This feature helps identify:

- **Natural pauses** and breath breaks in speech
- **Editing cut points** where content was removed
- **Separate takes** or recording sessions
- **Background noise periods** with no speech

### Silence Markers

Silence periods are represented as special entries in the words array:

```json
{
  "word": "**SILENCE**",
  "speaker": "**SILENCE**",
  "frame_in": 45,
  "frame_out": 70,
  "confidence": 1.0,
  "duration_ms": 1000
}
```

### Configuration

- **silence_threshold_ms**: Configurable threshold (default: 1000ms)
- **Detection Logic**: Gaps between words longer than the threshold are marked
- **Frame Accuracy**: Silence markers use precise frame boundaries
- **Exclusion from Speech**: Silence markers should be excluded when processing speech content

